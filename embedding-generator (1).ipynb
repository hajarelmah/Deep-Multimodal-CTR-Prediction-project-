{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":13875146,"datasetId":8820441,"databundleVersionId":14639025}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"****Multimodal Embedding Generator****\n\nThis notebook implements a **multimodal embedding** pipeline that combines textual and visual information to create rich item representations. The goal is to generate 128-dimensional embeddings for 91,718 items by fusing:\n\n* Text embeddings from item titles using Sentence-BERT (all-MiniLM-L6-v2)\n* Image embeddings from product photos using CLIP (ResNet-50)\n* Dimensionality reduction via PCA to compress from 1,408 to 128 dimensions","metadata":{}},{"cell_type":"code","source":"!pip uninstall -y protobuf\n!pip install protobuf==3.20.3\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T12:49:29.894519Z","iopub.execute_input":"2025-11-25T12:49:29.894835Z","iopub.status.idle":"2025-11-25T12:49:36.583412Z","shell.execute_reply.started":"2025-11-25T12:49:29.894811Z","shell.execute_reply":"2025-11-25T12:49:36.582478Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: protobuf 6.33.0\nUninstalling protobuf-6.33.0:\n  Successfully uninstalled protobuf-6.33.0\nCollecting protobuf==3.20.3\n  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\nDownloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: protobuf\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 3.20.3 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ntensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed protobuf-3.20.3\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install git+https://github.com/openai/CLIP.git\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T12:49:46.227298Z","iopub.execute_input":"2025-11-25T12:49:46.227938Z","iopub.status.idle":"2025-11-25T12:50:56.347240Z","shell.execute_reply.started":"2025-11-25T12:49:46.227905Z","shell.execute_reply":"2025-11-25T12:50:56.346296Z"}},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/openai/CLIP.git\n  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-2k7mclun\n  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-2k7mclun\n  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting ftfy (from clip==1.0)\n  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (25.0)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2025.11.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.20.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.10.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->clip==1.0)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->clip==1.0)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->clip==1.0)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch->clip==1.0)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch->clip==1.0)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch->clip==1.0)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch->clip==1.0)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch->clip==1.0)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch->clip==1.0)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch->clip==1.0)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->clip==1.0) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->clip==1.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->clip==1.0) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision->clip==1.0) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision->clip==1.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision->clip==1.0) (2024.2.0)\nDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m95.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: clip\n  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=6c3e275bb2a2a59734fdf88e108dbde26c3c49f37318441c2fe4fb3834944ddd\n  Stored in directory: /tmp/pip-ephem-wheel-cache-6u7wi_iy/wheels/3f/7c/a4/9b490845988bf7a4db33674d52f709f088f64392063872eb9a\nSuccessfully built clip\nInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ftfy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, clip\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed clip-1.0 ftfy-6.3.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Imports\nimport os\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nimport torch\nimport clip  # <<--- IMPORTANT: make sure this is imported\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.decomposition import PCA\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T12:51:00.830095Z","iopub.execute_input":"2025-11-25T12:51:00.830368Z","iopub.status.idle":"2025-11-25T12:51:35.267055Z","shell.execute_reply.started":"2025-11-25T12:51:00.830341Z","shell.execute_reply":"2025-11-25T12:51:35.266467Z"}},"outputs":[{"name":"stderr","text":"2025-11-25 12:51:14.296486: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764075074.494035      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764075074.550344      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"**Load the Microlens dataset containing item metadata and features.**\n*Note*: I have created a Kaggle dataset named **\"microlens\"** that contains all the required files for this notebook. Make sure to add it as a data source to your notebook before running.\nData Structure:\n\nitem_info.parquet (91,718 rows): Base item information with item_id, tags, and existing embeddings\n\nitem_feature.parquet (91,717 rows): Extended features including item_title and other attributes\n\nitem_images/: Folder containing product images named as image{item_id}.jpg","metadata":{}},{"cell_type":"code","source":"# ==========================\n# Paths\n# ==========================\nitem_info_path = \"/kaggle/input/microlens/item_info.parquet\"\nitem_feature_path = \"/kaggle/input/microlens/item_feature.parquet\"\nimage_folder = \"/kaggle/input/microlens/item_images/item_images/\"\n\noutput_path = \"/kaggle/working/item_info_fused_multimodal.parquet\"\n\n# ==========================\n# Load data\n# ==========================\nitem_info = pd.read_parquet(item_info_path)\nitem_feature = pd.read_parquet(item_feature_path)\n\nprint(f\"item_info shape: {item_info.shape}\")\nprint(f\"item_feature shape: {item_feature.shape}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-25T12:51:43.957855Z","iopub.execute_input":"2025-11-25T12:51:43.958763Z","iopub.status.idle":"2025-11-25T12:51:53.435195Z","shell.execute_reply.started":"2025-11-25T12:51:43.958736Z","shell.execute_reply":"2025-11-25T12:51:53.434526Z"}},"outputs":[{"name":"stdout","text":"item_info shape: (91718, 3)\nitem_feature shape: (91717, 7)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ==========================\n# Load models\n# ==========================\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Sentence-BERT for text\ntext_model = SentenceTransformer('all-MiniLM-L6-v2')\ntext_model.eval()\n\n# CLIP-RN50 for images\nclip_model, preprocess = clip.load(\"RN50\", device=device)\nclip_model.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T12:52:07.498000Z","iopub.execute_input":"2025-11-25T12:52:07.498719Z","iopub.status.idle":"2025-11-25T12:52:20.929998Z","shell.execute_reply.started":"2025-11-25T12:52:07.498684Z","shell.execute_reply":"2025-11-25T12:52:20.929233Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a5af8a1db4345828870ce2268aa2df3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6871c14a37464227a264413b8e92ceff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc52753f79d947919ce603443c980051"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df66ae9f8c194e558309bf0ae1d3a7bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a53b4a4037324ccc91a589d7f3c05e82"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0f5ff56ea7d4cbbbaa56103e3094835"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a806c9bf712941edbc216adb98634cd3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d4d0b702ee94b39adaf6267a5afad3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"236d536496c24033bb9f2ee11600b904"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a9562bc6cd64cd9b074aa29d1e0daf8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"760a0e3524ee4d37a379c783baf46fc7"}},"metadata":{}},{"name":"stderr","text":"100%|████████████████████████████████████████| 244M/244M [00:02<00:00, 111MiB/s]\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"CLIP(\n  (visual): ModifiedResNet(\n    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu1): ReLU(inplace=True)\n    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu2): ReLU(inplace=True)\n    (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu3): ReLU(inplace=True)\n    (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n    (layer1): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (avgpool): Identity()\n        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu3): ReLU(inplace=True)\n        (downsample): Sequential(\n          (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)\n          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (avgpool): Identity()\n        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu3): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (avgpool): Identity()\n        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu3): ReLU(inplace=True)\n      )\n    )\n    (layer2): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu3): ReLU(inplace=True)\n        (downsample): Sequential(\n          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (avgpool): Identity()\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu3): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (avgpool): Identity()\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu3): ReLU(inplace=True)\n      )\n      (3): Bottleneck(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (avgpool): Identity()\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu3): ReLU(inplace=True)\n      )\n    )\n    (layer3): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu3): ReLU(inplace=True)\n        (downsample): Sequential(\n          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (avgpool): Identity()\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu3): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (avgpool): Identity()\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu3): ReLU(inplace=True)\n      )\n      (3): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (avgpool): Identity()\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu3): ReLU(inplace=True)\n      )\n      (4): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (avgpool): Identity()\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu3): ReLU(inplace=True)\n      )\n      (5): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (avgpool): Identity()\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu3): ReLU(inplace=True)\n      )\n    )\n    (layer4): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu3): ReLU(inplace=True)\n        (downsample): Sequential(\n          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (avgpool): Identity()\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu3): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (avgpool): Identity()\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu3): ReLU(inplace=True)\n      )\n    )\n    (attnpool): AttentionPool2d(\n      (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n      (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n      (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n      (c_proj): Linear(in_features=2048, out_features=1024, bias=True)\n    )\n  )\n  (transformer): Transformer(\n    (resblocks): Sequential(\n      (0): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (gelu): QuickGELU()\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (1): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (gelu): QuickGELU()\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (2): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (gelu): QuickGELU()\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (3): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (gelu): QuickGELU()\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (4): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (gelu): QuickGELU()\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (5): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (gelu): QuickGELU()\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (6): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (gelu): QuickGELU()\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (7): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (gelu): QuickGELU()\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (8): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (gelu): QuickGELU()\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (9): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (gelu): QuickGELU()\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (10): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (gelu): QuickGELU()\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n      (11): ResidualAttentionBlock(\n        (attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (gelu): QuickGELU()\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n  (token_embedding): Embedding(49408, 512)\n  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n)"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# Encode text\n# ==========================\ntitles = item_feature['item_title'].tolist()\ntext_emb = text_model.encode(titles, batch_size=256, show_progress_bar=True)\n\n# ==========================\n# Encode images\n# ==========================\ndef encode_image(img_path):\n    \"\"\"Returns CLIP embedding for a single image\"\"\"\n    try:\n        img = preprocess(Image.open(img_path).convert('RGB')).unsqueeze(0).to(device)\n        with torch.no_grad():\n            emb = clip_model.encode_image(img)\n        return emb.cpu().numpy().flatten()\n    except:\n        # If image not found or corrupt, return zeros\n        return np.zeros(clip_model.visual.output_dim)\n\nimage_embs = []\nfor item_id in item_feature['item_id']:\n    img_file = os.path.join(image_folder, f\"image{item_id}.jpg\")  # adjust filename if needed\n    image_embs.append(encode_image(img_file))\n\nimage_embs = np.array(image_embs)\nprint(f\"Image embeddings shape: {image_embs.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T12:52:35.990350Z","iopub.execute_input":"2025-11-25T12:52:35.990634Z","iopub.status.idle":"2025-11-25T12:54:19.354403Z","shell.execute_reply.started":"2025-11-25T12:52:35.990614Z","shell.execute_reply":"2025-11-25T12:54:19.353650Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/359 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b22c1f916194115b1bcab792eee1c75"}},"metadata":{}},{"name":"stdout","text":"Image embeddings shape: (91717, 1024)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"print(item_info.columns)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T12:54:25.477809Z","iopub.execute_input":"2025-11-25T12:54:25.478342Z","iopub.status.idle":"2025-11-25T12:54:25.484132Z","shell.execute_reply.started":"2025-11-25T12:54:25.478320Z","shell.execute_reply":"2025-11-25T12:54:25.483312Z"}},"outputs":[{"name":"stdout","text":"Index(['item_id', 'item_tags', 'item_emb_d128'], dtype='object')\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# ==========================\n# Combine text + image embeddings\n# ==========================\n# Make sure we use all rows from item_feature\ntext_emb = text_model.encode(item_feature['item_title'].tolist(), batch_size=256, show_progress_bar=True)\n\nimage_embs = []\nfor item_id in item_feature['item_id']:\n    img_file = os.path.join(image_folder, f\"image{item_id}.jpg\")\n    image_embs.append(encode_image(img_file))\nimage_embs = np.array(image_embs)\n\nmultimodal_emb = np.concatenate([text_emb, image_embs], axis=1)\nprint(f\"Combined embeddings shape: {multimodal_emb.shape}\")  # should match item_info rows minus padding\n\n# ==========================\n# Add padding row at top\n# ==========================\npadding_row = np.zeros(multimodal_emb.shape[1])  # zeros for first row\nmultimodal_emb_full = np.vstack([padding_row, multimodal_emb])\nprint(f\"After padding, shape: {multimodal_emb_full.shape}\")  # now matches item_info rows\n\n# ==========================\n# PCA to 128-d\n# ==========================\npca = PCA(n_components=128)\nmultimodal_emb_128 = pca.fit_transform(multimodal_emb_full)\nprint(f\"PCA-reduced embeddings shape: {multimodal_emb_128.shape}\")\n\n# ==========================\n# Update item_info and save\n# ==========================\nitem_info['item_emb_d128'] = list(multimodal_emb_128)\nitem_info.to_parquet(output_path, index=False)\nprint(f\"Multimodal item_info saved to {output_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T12:54:28.797663Z","iopub.execute_input":"2025-11-25T12:54:28.798457Z","iopub.status.idle":"2025-11-25T12:55:21.160826Z","shell.execute_reply.started":"2025-11-25T12:54:28.798431Z","shell.execute_reply":"2025-11-25T12:55:21.160011Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/359 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e367d0381a0476c9b91388be70ad073"}},"metadata":{}},{"name":"stdout","text":"Combined embeddings shape: (91717, 1408)\nAfter padding, shape: (91718, 1408)\nPCA-reduced embeddings shape: (91718, 128)\nMultimodal item_info saved to /kaggle/working/item_info_fused_multimodal.parquet\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"\n# Load the fused multimodal item_info\nitem_info_fused = pd.read_parquet(\"/kaggle/working/item_info_fused_multimodal.parquet\")\n\n# See the first few rows\nprint(item_info_fused.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T12:55:25.727738Z","iopub.execute_input":"2025-11-25T12:55:25.728439Z","iopub.status.idle":"2025-11-25T12:55:26.216234Z","shell.execute_reply.started":"2025-11-25T12:55:25.728418Z","shell.execute_reply":"2025-11-25T12:55:26.215538Z"}},"outputs":[{"name":"stdout","text":"   item_id        item_tags                                      item_emb_d128\n0        0  [0, 0, 0, 0, 0]  [-0.0930395362406103, -0.023277998403371275, 0...\n1        1  [0, 0, 0, 0, 1]  [-0.012815042788755983, -0.09739979563827816, ...\n2        2  [0, 0, 2, 3, 4]  [-0.04312385186445468, 0.007002069472291313, -...\n3        3  [0, 0, 5, 6, 7]  [0.11733471775338727, -0.07496686939378722, -0...\n4        4  [0, 0, 0, 8, 9]  [0.029414119909678172, -0.028581324888330612, ...\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Check the shape\nprint(\"Shape:\", item_info_fused.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T12:55:31.177990Z","iopub.execute_input":"2025-11-25T12:55:31.178278Z","iopub.status.idle":"2025-11-25T12:55:31.182373Z","shell.execute_reply.started":"2025-11-25T12:55:31.178258Z","shell.execute_reply":"2025-11-25T12:55:31.181626Z"}},"outputs":[{"name":"stdout","text":"Shape: (91718, 3)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Inspect the embedding column (first row only, as it's large)\nprint(\"First embedding vector:\", item_info_fused['item_emb_d128'].iloc[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T12:55:38.117597Z","iopub.execute_input":"2025-11-25T12:55:38.118276Z","iopub.status.idle":"2025-11-25T12:55:38.123164Z","shell.execute_reply.started":"2025-11-25T12:55:38.118249Z","shell.execute_reply":"2025-11-25T12:55:38.122580Z"}},"outputs":[{"name":"stdout","text":"First embedding vector: [-9.30395362e-02 -2.32779984e-02  2.04750604e-01 -1.36218245e-01\n  4.18094425e-02 -5.29150630e-02  4.71030199e-02  4.25416613e-02\n -4.98953277e-02 -5.23277428e-02  2.04498765e-02 -1.21913106e-02\n  1.11224409e-02  2.01866605e-02  5.27886647e-03  1.03064325e-02\n -4.86732067e-02  7.22326592e-02  1.53338971e-02 -4.80879004e-02\n -4.73850613e-02 -1.37578837e-02  1.77517524e-02  2.07183565e-02\n  7.84526965e-03 -7.24601752e-02  3.07343365e-04  5.18291762e-02\n  8.13975265e-03 -2.34206490e-03 -4.06716391e-02  1.89334248e-02\n  2.35710335e-02  2.93670449e-03  1.17712800e-02 -3.46356395e-02\n -4.27335892e-03  2.18394862e-02  1.59552658e-02  2.42888382e-02\n  6.87711665e-03 -7.01798813e-03  4.11477817e-03 -1.97578338e-02\n -1.13116516e-02 -4.18479367e-02 -1.24200766e-02  5.30512977e-02\n -1.86224218e-02  6.18654530e-03  2.16350757e-02  1.46057335e-02\n -1.53896270e-03 -9.80813580e-03 -2.42925596e-02  9.57079043e-03\n  1.41696380e-02  2.36932926e-02  1.97235621e-02  3.95591688e-03\n  3.69203560e-02  2.40705494e-02 -2.58162075e-02  2.32999668e-03\n -1.37682015e-02 -3.99228444e-03  1.67526531e-02  9.13928661e-03\n  2.03948037e-02 -2.22319150e-02 -4.13533727e-03  5.34457389e-03\n -2.76665491e-02  3.38827318e-02 -3.25703910e-03 -6.00229587e-03\n -1.10494800e-02 -7.23235826e-03  1.76586436e-02 -1.14910949e-03\n -4.28870892e-03 -6.17352272e-03  5.36388719e-02 -1.97709092e-02\n -1.83950670e-02 -3.14520933e-02  2.74732630e-02  4.83764901e-03\n  1.63501164e-02  4.63161444e-03  3.89169294e-02  4.02163409e-02\n  8.78171799e-03  5.73217692e-03  9.30542921e-03 -6.67701729e-03\n -2.27084329e-02  3.02621192e-02 -1.32413191e-02  4.45842222e-03\n -3.72838104e-04  8.32321744e-03  2.12932880e-03 -7.24269159e-03\n -4.42952587e-03  1.60326008e-02  1.06755870e-02  1.97636318e-02\n  1.73611163e-02  3.46432320e-02 -2.52260080e-03 -1.27735813e-03\n  2.03931564e-02 -2.25166911e-02  3.89936155e-05 -1.12281096e-05\n -5.40439581e-03 -1.51541563e-02  5.67611378e-03  2.32595943e-03\n -1.44159187e-02  2.57469778e-02  6.85013932e-03 -1.52183511e-02\n  9.75561854e-03 -7.44640350e-03 -1.65600652e-02  6.25497462e-03]\n","output_type":"stream"}],"execution_count":11}]}