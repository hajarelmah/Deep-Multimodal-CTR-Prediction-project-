{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13875146,"sourceType":"datasetVersion","datasetId":8820441}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# =======================\n# BLOCK 1 - IMPORTS\n# =======================\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import roc_auc_score\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom collections import Counter\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-17T09:25:08.343659Z","iopub.execute_input":"2025-12-17T09:25:08.344227Z","iopub.status.idle":"2025-12-17T09:25:13.728589Z","shell.execute_reply.started":"2025-12-17T09:25:08.344188Z","shell.execute_reply":"2025-12-17T09:25:13.728014Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# =======================\n# BLOCK 2 - LOAD DATA\n# =======================\nPATH = \"/kaggle/input/microlens/\"\n\ntrain = pd.read_parquet(PATH + \"train.parquet\")\nvalid = pd.read_parquet(PATH + \"valid.parquet\")\ntest = pd.read_parquet(PATH + \"test.parquet\")\nitem_info = pd.read_parquet(PATH + \"item_info_fused_multimodal.parquet\")\n\nprint(\"Train shape:\", train.shape)\nprint(\"Valid shape:\", valid.shape)\nprint(\"Test shape:\", test.shape)\nprint(\"Item info shape:\", item_info.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T09:25:20.927179Z","iopub.execute_input":"2025-12-17T09:25:20.927990Z","iopub.status.idle":"2025-12-17T09:25:31.641323Z","shell.execute_reply.started":"2025-12-17T09:25:20.927953Z","shell.execute_reply":"2025-12-17T09:25:31.640667Z"}},"outputs":[{"name":"stdout","text":"Train shape: (3600000, 6)\nValid shape: (10000, 6)\nTest shape: (379142, 6)\nItem info shape: (91718, 3)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# =======================\n# BLOCK 3 - PREPARE ITEM EMBEDDINGS\n# =======================\nprint(\"\\nPreparing item embeddings...\")\nitem_info = item_info[~item_info[\"item_emb_d128\"].isna()].reset_index(drop=True)\nemb_matrix = np.vstack(item_info[\"item_emb_d128\"].values)\n\n# Simple normalization\nemb_matrix = (emb_matrix - emb_matrix.mean(axis=0)) / (emb_matrix.std(axis=0) + 1e-8)\n\nprint(f\"Item embedding shape: {emb_matrix.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T09:26:09.062764Z","iopub.execute_input":"2025-12-17T09:26:09.063037Z","iopub.status.idle":"2025-12-17T09:26:09.331003Z","shell.execute_reply.started":"2025-12-17T09:26:09.063015Z","shell.execute_reply":"2025-12-17T09:26:09.330284Z"}},"outputs":[{"name":"stdout","text":"\nPreparing item embeddings...\nItem embedding shape: (91718, 128)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# =======================\n# BLOCK 4 - CREATE ITEM EMBEDDING TENSOR\n# =======================\n# Create a mapping from item_id to embedding index\nitem_id_to_idx = {item_id: idx for idx, item_id in enumerate(item_info['item_id'].values)}\n\n# Pad with zeros for unknown items\nmax_item_id = max(train['item_id'].max(), valid['item_id'].max(), test['item_id'].max())\nprint(f\"Max item_id: {max_item_id}\")\n\n# Create full embedding matrix with padding\nfull_emb_matrix = np.zeros((max_item_id + 1, 128), dtype=np.float32)\nfor item_id, idx in item_id_to_idx.items():\n    full_emb_matrix[item_id] = emb_matrix[idx]\n\nprint(f\"Full embedding matrix shape: {full_emb_matrix.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T09:26:19.714371Z","iopub.execute_input":"2025-12-17T09:26:19.715238Z","iopub.status.idle":"2025-12-17T09:26:19.839722Z","shell.execute_reply.started":"2025-12-17T09:26:19.715200Z","shell.execute_reply":"2025-12-17T09:26:19.839039Z"}},"outputs":[{"name":"stdout","text":"Max item_id: 91717\nFull embedding matrix shape: (91718, 128)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# =======================\n# BLOCK 5 - DATASET\n# =======================\nclass CTRDataset(Dataset):\n    def __init__(self, df, is_test=False):\n        self.user_id = df[\"user_id\"].values\n        self.item_id = df[\"item_id\"].values\n        self.item_seq = df[\"item_seq\"].values\n        self.likes = df[\"likes_level\"].values\n        self.views = df[\"views_level\"].values\n        self.is_test = is_test\n        \n        if not is_test:\n            self.label = df[\"label\"].values\n        else:\n            self.ID = df[\"ID\"].values\n\n    def __len__(self):\n        return len(self.user_id)\n\n    def __getitem__(self, idx):\n        data = {\n            \"user_id\": torch.tensor(self.user_id[idx], dtype=torch.long),\n            \"item_id\": torch.tensor(self.item_id[idx], dtype=torch.long),\n            \"item_seq\": torch.tensor(self.item_seq[idx], dtype=torch.long),\n            \"likes\": torch.tensor(self.likes[idx], dtype=torch.long),\n            \"views\": torch.tensor(self.views[idx], dtype=torch.long),\n        }\n        \n        if self.is_test:\n            data[\"ID\"] = self.ID[idx]\n        else:\n            data[\"label\"] = torch.tensor(self.label[idx], dtype=torch.float32)\n        \n        return data\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T09:26:55.028984Z","iopub.execute_input":"2025-12-17T09:26:55.029564Z","iopub.status.idle":"2025-12-17T09:26:55.036245Z","shell.execute_reply.started":"2025-12-17T09:26:55.029513Z","shell.execute_reply":"2025-12-17T09:26:55.035592Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# =======================\n# BLOCK 6 - DATALOADERS\n# =======================\ntrain_ds = CTRDataset(train, is_test=False)\nvalid_ds = CTRDataset(valid, is_test=False)\ntest_ds = CTRDataset(test, is_test=True)\n\ntrain_loader = DataLoader(train_ds, batch_size=2048, shuffle=True, num_workers=2)\nvalid_loader = DataLoader(valid_ds, batch_size=2048, shuffle=False, num_workers=2)\ntest_loader = DataLoader(test_ds, batch_size=2048, shuffle=False, num_workers=2)\n\nprint(\"Dataloaders ready!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T09:27:09.153480Z","iopub.execute_input":"2025-12-17T09:27:09.154166Z","iopub.status.idle":"2025-12-17T09:27:09.159885Z","shell.execute_reply.started":"2025-12-17T09:27:09.154139Z","shell.execute_reply":"2025-12-17T09:27:09.159280Z"}},"outputs":[{"name":"stdout","text":"Dataloaders ready!\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# =======================\n# BLOCK 7 - ATTENTION POOLING MODULE\n# =======================\nclass AttentionPooling(nn.Module):\n    \"\"\"Attention-based pooling for sequence embeddings\"\"\"\n    def __init__(self, emb_dim):\n        super().__init__()\n        self.attention = nn.Sequential(\n            nn.Linear(emb_dim, emb_dim // 2),\n            nn.Tanh(),\n            nn.Linear(emb_dim // 2, 1)\n        )\n    \n    def forward(self, seq_emb, mask):\n        # seq_emb: [batch, seq_len, emb_dim]\n        # mask: [batch, seq_len, 1]\n        attn_scores = self.attention(seq_emb)  # [batch, seq_len, 1]\n        attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n        attn_weights = F.softmax(attn_scores, dim=1)\n        pooled = (seq_emb * attn_weights).sum(dim=1)\n        return pooled","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T09:27:27.646218Z","iopub.execute_input":"2025-12-17T09:27:27.646495Z","iopub.status.idle":"2025-12-17T09:27:27.651865Z","shell.execute_reply.started":"2025-12-17T09:27:27.646472Z","shell.execute_reply":"2025-12-17T09:27:27.651114Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# =======================\n# BLOCK 8 - IMPROVED MODEL\n# =======================\nclass ImprovedMMCTRModel(nn.Module):\n    def __init__(self, num_users, num_items, item_emb_matrix, emb_dim=64):\n        super().__init__()\n        \n        # Increased embedding dimension for better representation\n        self.emb_dim = emb_dim\n        \n        # Learnable embeddings with better initialization\n        self.user_emb = nn.Embedding(num_users + 1, emb_dim, padding_idx=0)\n        self.item_emb = nn.Embedding(num_items, emb_dim, padding_idx=0)\n        \n        # Categorical feature embeddings\n        self.likes_emb = nn.Embedding(11, emb_dim // 2)\n        self.views_emb = nn.Embedding(11, emb_dim // 2)\n        \n        # Pre-trained multimodal embeddings\n        self.register_buffer('item_mm_emb', torch.tensor(item_emb_matrix, dtype=torch.float32))\n        \n        # Transform multimodal embeddings to match dimension\n        self.mm_transform = nn.Sequential(\n            nn.Linear(128, emb_dim),\n            nn.LayerNorm(emb_dim),\n            nn.ReLU(),\n            nn.Dropout(0.2)\n        )\n        \n        # Attention pooling for sequence\n        self.seq_attention = AttentionPooling(emb_dim)\n        \n        # Feature interaction layers\n        self.user_item_interaction = nn.Bilinear(emb_dim, emb_dim, emb_dim)\n        self.seq_item_interaction = nn.Bilinear(emb_dim, emb_dim, emb_dim)\n        \n        # Calculate total feature dimension\n        # user(64) + item(64) + seq(64) + likes(32) + views(32) + mm_transformed(64)\n        # + user_item_interact(64) + seq_item_interact(64) = 448\n        total_dim = emb_dim * 6 + (emb_dim // 2) * 2\n        \n        # Deep MLP with batch normalization and residual connections\n        self.bn1 = nn.BatchNorm1d(total_dim)\n        self.fc1 = nn.Linear(total_dim, 512)\n        self.bn2 = nn.BatchNorm1d(512)\n        self.fc2 = nn.Linear(512, 256)\n        self.bn3 = nn.BatchNorm1d(256)\n        self.fc3 = nn.Linear(256, 128)\n        self.bn4 = nn.BatchNorm1d(128)\n        self.fc_out = nn.Linear(128, 1)\n        \n        self.dropout = nn.Dropout(0.3)\n        \n        # Xavier initialization\n        self._init_weights()\n    \n    def _init_weights(self):\n        nn.init.xavier_normal_(self.user_emb.weight)\n        nn.init.xavier_normal_(self.item_emb.weight)\n        nn.init.xavier_normal_(self.likes_emb.weight)\n        nn.init.xavier_normal_(self.views_emb.weight)\n        \n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n    \n    def forward(self, user_id, item_id, item_seq, likes, views):\n        # Basic embeddings\n        user_e = self.user_emb(user_id)\n        item_e = self.item_emb(item_id)\n        \n        # Sequence with attention pooling\n        seq_e = self.item_emb(item_seq)\n        mask = (item_seq != 0).unsqueeze(-1).float()\n        seq_e = self.seq_attention(seq_e, mask)\n        \n        # Categorical features\n        likes_e = self.likes_emb(likes)\n        views_e = self.views_emb(views)\n        \n        # Multimodal embeddings with transformation\n        item_mm_e = self.item_mm_emb[item_id]\n        item_mm_e = self.mm_transform(item_mm_e)\n        \n        # Feature interactions\n        user_item_cross = self.user_item_interaction(user_e, item_e)\n        seq_item_cross = self.seq_item_interaction(seq_e, item_e)\n        \n        # Concatenate all features\n        x = torch.cat([\n            user_e, item_e, seq_e, likes_e, views_e, item_mm_e,\n            user_item_cross, seq_item_cross\n        ], dim=1)\n        \n        # Deep network with residual connections\n        x = self.bn1(x)\n        x1 = F.relu(self.bn2(self.fc1(x)))\n        x1 = self.dropout(x1)\n        \n        x2 = F.relu(self.bn3(self.fc2(x1)))\n        x2 = self.dropout(x2)\n        \n        x3 = F.relu(self.bn4(self.fc3(x2)))\n        x3 = self.dropout(x3)\n        \n        logits = self.fc_out(x3).squeeze(1)\n        \n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T09:27:46.735098Z","iopub.execute_input":"2025-12-17T09:27:46.735389Z","iopub.status.idle":"2025-12-17T09:27:46.747505Z","shell.execute_reply.started":"2025-12-17T09:27:46.735364Z","shell.execute_reply":"2025-12-17T09:27:46.746867Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# =======================\n# BLOCK 9 - LABEL SMOOTHING LOSS\n# =======================\nclass LabelSmoothingBCELoss(nn.Module):\n    def __init__(self, smoothing=0.05):\n        super().__init__()\n        self.smoothing = smoothing\n    \n    def forward(self, logits, targets):\n        # Smooth labels: 0 -> smoothing/2, 1 -> 1-smoothing/2\n        targets = targets * (1 - self.smoothing) + 0.5 * self.smoothing\n        return F.binary_cross_entropy_with_logits(logits, targets)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T09:28:08.045367Z","iopub.execute_input":"2025-12-17T09:28:08.045694Z","iopub.status.idle":"2025-12-17T09:28:08.051397Z","shell.execute_reply.started":"2025-12-17T09:28:08.045667Z","shell.execute_reply":"2025-12-17T09:28:08.050482Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# =======================\n# BLOCK 10 - TRAINING SETUP\n# =======================\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"\\nUsing device: {device}\")\n\nnum_users = train[\"user_id\"].max()\nnum_items = full_emb_matrix.shape[0]\n\nprint(f\"Num users: {num_users}, Num items: {num_items}\")\n\nmodel = ImprovedMMCTRModel(\n    num_users=num_users,\n    num_items=num_items,\n    item_emb_matrix=full_emb_matrix,\n    emb_dim=64  # Increased from 32\n).to(device)\n\n# Label smoothing loss for regularization\ncriterion = LabelSmoothingBCELoss(smoothing=0.05)\n\n# Separate learning rates for different components\npretrained_params = []\nother_params = []\n\nfor name, param in model.named_parameters():\n    if 'mm_transform' in name:\n        pretrained_params.append(param)\n    else:\n        other_params.append(param)\n\noptimizer = torch.optim.AdamW([\n    {'params': other_params, 'lr': 2e-3, 'weight_decay': 1e-4},\n    {'params': pretrained_params, 'lr': 5e-4, 'weight_decay': 1e-5}\n])\n\n# Cosine annealing with warm restarts\nscheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n    optimizer, T_0=5, T_mult=2, eta_min=1e-6\n)\n\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T09:28:24.674421Z","iopub.execute_input":"2025-12-17T09:28:24.675147Z","iopub.status.idle":"2025-12-17T09:28:28.709685Z","shell.execute_reply.started":"2025-12-17T09:28:24.675119Z","shell.execute_reply":"2025-12-17T09:28:28.708939Z"}},"outputs":[{"name":"stdout","text":"\nUsing device: cuda\nNum users: 1000000, Num items: 91718\nModel parameters: 70,802,562\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# =======================\n# BLOCK 11 - TRAINING LOOP\n# =======================\nEPOCHS = 30\nbest_val_auc = 0\npatience = 7  # Increased patience\npatience_counter = 0\n\nprint(\"\\nüöÄ Starting enhanced training...\")\nfor epoch in range(EPOCHS):\n    # Training\n    model.train()\n    total_loss = 0\n    train_preds = []\n    train_labels = []\n    \n    for batch in train_loader:\n        user_id = batch[\"user_id\"].to(device)\n        item_id = batch[\"item_id\"].to(device)\n        item_seq = batch[\"item_seq\"].to(device)\n        likes = batch[\"likes\"].to(device)\n        views = batch[\"views\"].to(device)\n        label = batch[\"label\"].to(device)\n        \n        optimizer.zero_grad()\n        \n        logits = model(user_id, item_id, item_seq, likes, views)\n        loss = criterion(logits, label)\n        \n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        \n        total_loss += loss.item()\n        \n        with torch.no_grad():\n            probs = torch.sigmoid(logits)\n            train_preds.extend(probs.cpu().numpy())\n            train_labels.extend(label.cpu().numpy())\n    \n    # Step scheduler\n    scheduler.step()\n    \n    train_auc = roc_auc_score(train_labels, train_preds)\n    avg_loss = total_loss / len(train_loader)\n    \n    # Validation\n    model.eval()\n    val_preds = []\n    val_labels = []\n    \n    with torch.no_grad():\n        for batch in valid_loader:\n            user_id = batch[\"user_id\"].to(device)\n            item_id = batch[\"item_id\"].to(device)\n            item_seq = batch[\"item_seq\"].to(device)\n            likes = batch[\"likes\"].to(device)\n            views = batch[\"views\"].to(device)\n            label = batch[\"label\"].to(device)\n            \n            logits = model(user_id, item_id, item_seq, likes, views)\n            probs = torch.sigmoid(logits)\n            \n            val_preds.extend(probs.cpu().numpy())\n            val_labels.extend(label.cpu().numpy())\n    \n    val_auc = roc_auc_score(val_labels, val_preds)\n    current_lr = optimizer.param_groups[0]['lr']\n    \n    print(f\"Epoch {epoch+1:02d}/{EPOCHS} | Loss: {avg_loss:.4f} | \"\n          f\"Train AUC: {train_auc:.4f} | Val AUC: {val_auc:.4f} | LR: {current_lr:.6f}\")\n    \n    # Save best model\n    if val_auc > best_val_auc:\n        best_val_auc = val_auc\n        patience_counter = 0\n        torch.save(model.state_dict(), 'best_model.pt')\n        print(f\"‚úÖ New best! Val AUC: {val_auc:.4f}\")\n    else:\n        patience_counter += 1\n        print(f\"‚è≥ No improvement. Patience {patience_counter}/{patience}\")\n        \n        if patience_counter >= patience:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n\nmodel.load_state_dict(torch.load('best_model.pt'))\nprint(f\"\\nüéØ Best Validation AUC: {best_val_auc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T09:28:50.991204Z","iopub.execute_input":"2025-12-17T09:28:50.991870Z","iopub.status.idle":"2025-12-17T10:04:28.910034Z","shell.execute_reply.started":"2025-12-17T09:28:50.991841Z","shell.execute_reply":"2025-12-17T10:04:28.909230Z"}},"outputs":[{"name":"stdout","text":"\nüöÄ Starting enhanced training...\nEpoch 01/30 | Loss: 0.2319 | Train AUC: 0.9744 | Val AUC: 0.8868 | LR: 0.001809\n‚úÖ New best! Val AUC: 0.8868\nEpoch 02/30 | Loss: 0.1513 | Train AUC: 0.9958 | Val AUC: 0.9003 | LR: 0.001309\n‚úÖ New best! Val AUC: 0.9003\nEpoch 03/30 | Loss: 0.1287 | Train AUC: 0.9990 | Val AUC: 0.8975 | LR: 0.000692\n‚è≥ No improvement. Patience 1/7\nEpoch 04/30 | Loss: 0.1208 | Train AUC: 0.9997 | Val AUC: 0.9188 | LR: 0.000192\n‚úÖ New best! Val AUC: 0.9188\nEpoch 05/30 | Loss: 0.1188 | Train AUC: 0.9998 | Val AUC: 0.9251 | LR: 0.002000\n‚úÖ New best! Val AUC: 0.9251\nEpoch 06/30 | Loss: 0.1266 | Train AUC: 0.9993 | Val AUC: 0.8923 | LR: 0.001951\n‚è≥ No improvement. Patience 1/7\nEpoch 07/30 | Loss: 0.1266 | Train AUC: 0.9994 | Val AUC: 0.8904 | LR: 0.001809\n‚è≥ No improvement. Patience 2/7\nEpoch 08/30 | Loss: 0.1206 | Train AUC: 0.9998 | Val AUC: 0.8911 | LR: 0.001588\n‚è≥ No improvement. Patience 3/7\nEpoch 09/30 | Loss: 0.1194 | Train AUC: 0.9998 | Val AUC: 0.8652 | LR: 0.001309\n‚è≥ No improvement. Patience 4/7\nEpoch 10/30 | Loss: 0.1190 | Train AUC: 0.9999 | Val AUC: 0.8593 | LR: 0.001000\n‚è≥ No improvement. Patience 5/7\nEpoch 11/30 | Loss: 0.1188 | Train AUC: 0.9999 | Val AUC: 0.7784 | LR: 0.000692\n‚è≥ No improvement. Patience 6/7\nEpoch 12/30 | Loss: 0.1185 | Train AUC: 0.9999 | Val AUC: 0.8744 | LR: 0.000413\n‚è≥ No improvement. Patience 7/7\nEarly stopping at epoch 12\n\nüéØ Best Validation AUC: 0.9251\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# =======================\n# BLOCK 12 - GENERATE PREDICTIONS\n# =======================\nmodel.eval()\nall_ids = []\nall_preds = []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        user_id = batch[\"user_id\"].to(device)\n        item_id = batch[\"item_id\"].to(device)\n        item_seq = batch[\"item_seq\"].to(device)\n        likes = batch[\"likes\"].to(device)\n        views = batch[\"views\"].to(device)\n        \n        logits = model(user_id, item_id, item_seq, likes, views)\n        probs = torch.sigmoid(logits)\n        \n        all_preds.extend(probs.cpu().numpy())\n        all_ids.extend(batch[\"ID\"])\n\n# Create submission\nsubmission = pd.DataFrame({\n    \"ID\": all_ids,\n    \"Task1&2\": all_preds\n})\n\nsubmission = submission.sort_values(\"ID\").reset_index(drop=True)\nsubmission.to_csv(\"submission.csv\", index=False)\n\nprint(\"\\n‚úÖ Submission saved!\")\nprint(f\"Prediction stats - Min: {submission['Task1&2'].min():.6f}, Max: {submission['Task1&2'].max():.6f}, Mean: {submission['Task1&2'].mean():.6f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T10:05:31.538302Z","iopub.execute_input":"2025-12-17T10:05:31.538644Z","iopub.status.idle":"2025-12-17T10:06:18.392895Z","shell.execute_reply.started":"2025-12-17T10:05:31.538615Z","shell.execute_reply":"2025-12-17T10:06:18.392120Z"}},"outputs":[{"name":"stdout","text":"\n‚úÖ Submission saved!\nPrediction stats - Min: 0.013942, Max: 0.980519, Mean: 0.573859\n","output_type":"stream"}],"execution_count":14}]}